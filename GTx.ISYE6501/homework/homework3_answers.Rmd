---
title: "Homework 3 (ISYE 6501) — Questions 5.1 & 6.1 & 6.2"
output:
  pdf_document:
    latex_engine: xelatex
---

Sorry in advance to my peer reviewers! (ᵕ—ᴗ—) I like to write out my reasoning and can be... wordy... But I will try to be even more brief this time after taking the advice from HW2 peers!  
<!-- ## rmarkdown::render("edX/GTx.ISYE6501/homework/homework3_answers.Rmd") -->  
I want to thank https://r-graphics.org/ for guiding me on how to create clear and effective graphs and plots. While I did not copy any specific lines of code, I used it as a helpful reference when creating the plots for Question 6.2.  
I also want to thank R’s built in help() function, which has been extremely useful for understanding things or functions I do not immediately get, and for providing examples that helped me write my own, more complex scripts.  

## Question 5.1 Answer

```{r setup, echo=FALSE}
rm(list = ls())
library(outliers)
set.seed(16) #not needed but I added this to continue practicing adding a seed to my code blocks

crime_data <- read.table(
  "C:/Users/mstout/OneDrive - AANP/Documents/Workspace.MAIN/edX/GTx.ISYE6501/data/Homework3_ISYE_6501/data 5.1/uscrime.txt",
  header = TRUE
)
```
```{r grubbs, message=FALSE, warning=FALSE}
dim(crime_data)

g <- grubbs.test(na.omit(crime_data$Crime)) 
#type = 20 was not working for .Rmd file,
#so I had to use this after already knowing the higher value was the possible outlier
g
```
Even though they tell us exactly which test to run, I still wanted to know "why this test?"  
So, from my understanding, this specific kind of test is just to see if there is one unusually large or small value compared to the rest of the data, which is perfect for the question we are being asked.  

Analysis:  
[1993] is was the most extreme value in the data, and is extreme in the sense this is the largest value.  
I got [G = 2.81287], which is the Grubbs test statistic, this shows us that the largest value is [~2.81] standard deviations (distance) away from the mean.  
I also got [U = 0.82426], I didn't know what this meant, so I used 'help(grubbs.test)' which essentially told me:
1. Smaller U corresponds to a more extreme observation  
2. It can be ignored safely for this assignment  
FInally, I got [p-value = 0.07887] (7.87%), This result says: If all the crime rates follow the same general pattern and there is no true outlier, results this extreme would show up about 8 times out of 100 just by chance.  
So, in my personal opinion of this, there is no outlier in this dataset.  
8 out of 100 is not insanely rare, rare enough to notice, but not rare enough I could confidently say "this can't occur "naturally."  
This sentiment I believe is backed by the fact Grubbs is really good for catching the extremes, which could be the outlier, but not so good at catching multiple large values at once.  
The next closest value is [1969], which is pretty close. So statistically, the upper tail has more than one large value, which also weakens the case that [1993] is a lone, aberrant point.  

## Question 6.1 Answer

I believe a change detection model would be appropriate for monitoring my heart rate during a semi long run, since heart rate is measured continuously over time and usually stays around a normal level when my pace and conditions are consistent.  
During a recent 5 mile run at a 12:00 mile pace (I know I'm slow), my average heart rate was [159bpm], with a maximum of [169bpm]. So, a meaningful change to me would be when my heart rate starts to stay above my normal running level, which could be due to increased effort, fatigue, dehydration, or changes in terrain (uphill).  
Using the CUSUM technique, the baseline mean would be set to my typical heart rate of [159bpm].  
The threshold or reference value would be based on the smallest sustained increase I care about, maybe something like a prolonged [5/6bpm] rise above my average rather than short spikes.  
The critical value would be larger to avoid false alarms, so the model only signals when my heart rate stays high over time instead of reacting to a single high reading like [169bpm].  
This lets the model catch real changes while ignoring normal short term variation.  

## Question 6.2 Answer

```{r tempssetup, echo=FALSE}
path <- "C:/Users/mstout/OneDrive - AANP/Documents/Workspace.MAIN/edX/GTx.ISYE6501/data/Homework3_ISYE_6501/data 6.2/temps.txt"
w <- read.delim(path, sep = "\t", header = TRUE, check.names = FALSE)

print(dim(w))
```
```{r temps1table, echo=FALSE, message=FALSE, warning=FALSE}
years <- names(w)[names(w) != "DAY"]
d <- as.Date(paste0("2000-", w$DAY), format = "%Y-%d-%b")
mmdd <- format(d, "%m-%d")

L <- do.call(rbind, lapply(years, function(y) {
  data.frame(
    year = as.integer(y),
    date = as.Date(paste0(y, "-", mmdd)),
    tmax = as.numeric(w[[y]])
  )
}))
L <- L[order(L$year, L$date), ]
```
```{r temps2cusum, echo=FALSE, message=FALSE, warning=FALSE}
one_year <- function(z) {
  z <- z[order(z$date), ]
  x <- z$tmax
  s <- cumsum(x - mean(x, na.rm = TRUE))
  k <- which.max(s)
  data.frame(
    year = z$year[1],
    end_date = z$date[k],
    end_doy = as.integer(format(z$date[k], "%j")),
    mean_before = mean(x[1:k], na.rm = TRUE),
    mean_after = mean(x[(k + 1):length(x)], na.rm = TRUE)
  )
}
```
```{r temps3cusumyr, echo=FALSE, message=FALSE, warning=FALSE}
R <- do.call(rbind, lapply(split(L, L$year), one_year))
R$drop_after <- R$mean_after - R$mean_before
R <- R[order(R$year), ]
```
```{r temps4means, echo=FALSE, message=FALSE, warning=FALSE}
A <- aggregate(tmax ~ year, L, mean)
A <- A[order(A$year), ]
```
```{r temps52014, echo=FALSE, message=FALSE, warning=FALSE}
Z <- split(L, L$year)[["2014"]]
Z <- Z[order(Z$date), ]
S <- cumsum(Z$tmax - mean(Z$tmax, na.rm = TRUE))
k2014 <- which.max(S)
```
```{r temps6summaries, echo=TRUE, message=FALSE, warning=FALSE}
summary(lm(mean_before ~ year, data = R))
summary(lm(end_doy ~ year, data = R))
```
```{r temps7plots, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=7}
par(mfrow = c(2, 2))

plot(R$year, R$end_doy, xlab = "Year", ylab = "End of summer day of year")
abline(lm(end_doy ~ year, data = R))

plot(R$year, R$mean_before, xlab = "Year", ylab = "Mean before end date")
abline(lm(mean_before ~ year, data = R))

plot(A$year, A$tmax, xlab = "Jul to Oct mean high", ylab = "Temperature")

plot(Z$date, S, type = "l", xlab = "Date", ylab = "CUSUM")
abline(v = Z$date[k2014], lty = 2)
```
```{r temps8sanity, echo=TRUE, message=FALSE, warning=FALSE}
dim(w)
dim(L)
dim(R)
head(R,20) #just want to ensure it all loaded as expected
```
Using CUSUM, I centered each year’s daily high temperatures around that year’s July to October average and defined the CUSUM maximum as the end of summer.  
With this method, I found that summer in Atlanta typically ends in early to mid September, most often between about [September 5] and [September 20].  
Before the CUSUM peak, average daily highs were usually in the upper [80s] to low [90s], and after the peak they dropped to the mid [70s], a decrease of about [10 to 15] degrees.  

Analysis:  
To check whether summers have been lasting longer over time, I looked at the trend in the calendar day of year for the CUSUM end of summer date across years.  
The estimated temperature trend was about [0.05] degrees per year with a p value of about [0.58], and the trend in the end of summer date was about [-0.14] days per year with a p value of about [0.66].  
Both p values are fairly large, which means these trends are not statistically significant and could easily be due to random variation rather than a real change over time.  
Based on these results from [1996] to [2015], there is no strong statistical evidence that Atlanta’s summers became warmer or lasted longer over this period.  